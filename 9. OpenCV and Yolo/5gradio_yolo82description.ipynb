{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'response_audio.wav'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "from ultralytics import YOLO  \n",
    "  \n",
    "\n",
    "# #################### YOLO V8 ##############################\n",
    "\n",
    "# YOLOv8 모델 로드  \n",
    "model = YOLO('yolov8n.pt')  # YOLOv8 모델 파일 경로  \n",
    "  \n",
    "# 객체 감지 함수  \n",
    "def detect_objects(image):  \n",
    "    results = model(image)  \n",
    "      \n",
    "    # 결과를 처리하여 이미지에 사각형 그리기 및 레이블 표시  \n",
    "    labels = model.names  \n",
    "    for result in results:  \n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  \n",
    "        confidences = result.boxes.conf.cpu().numpy()  \n",
    "        class_ids = result.boxes.cls.cpu().numpy()  \n",
    "          \n",
    "        for i, box in enumerate(boxes):  \n",
    "            x1, y1, x2, y2 = map(int, box)  \n",
    "            confidence = confidences[i]  \n",
    "            class_id = int(class_ids[i])  \n",
    "            label = labels[class_id]  \n",
    "              \n",
    "            # 사각형 그리기  \n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  \n",
    "            # 레이블 표시  \n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)  \n",
    "      \n",
    "    return image      \n",
    "#############################################################\n",
    "\n",
    "################# chatgpt_response ##########################\n",
    "\n",
    "def chatgpt_response(image_array, history):\n",
    "    endpoint = \"https://fimtrus-openai.openai.azure.com\"\n",
    "    api_key = \"310a6832c2394daf97a3f446cc86ce20\"\n",
    "    deployment_name = \"fitmrus-gpt4o\"\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': api_key\n",
    "    }\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    #System\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"너는 사진 속에서 감지된 물체에 대해서 분석하는 봇이야.\"\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    image = Image.fromarray(image_array)\n",
    "    buffered_io = BytesIO()\n",
    "    image.save(buffered_io, format='png')\n",
    "    base64_image = base64.b64encode(buffered_io.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    # original_width, original_height = image.size\n",
    "    # ratio = 400 / original_width\n",
    "    # resized_image = image.resize((int(original_width * ratio), int(original_height * ratio)), Image.LANCOS)\n",
    "\n",
    "    #User\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"이 사진에서 감지된 물체에 대해 감지 확률과 함께 자세하게 설명해줘.\"\n",
    "        },{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-02-15-preview\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        bot_response = result['choices'][0]['message']['content'].strip()\n",
    "        history.append(('User', bot_response))\n",
    "        return history\n",
    "    else:\n",
    "        history.append((str(response.status_code), response.text))\n",
    "        return history\n",
    "#############################################################\n",
    "\n",
    "################# TTS #######################################\n",
    "\n",
    "def get_token():\n",
    "    endpoint = \"https://eastus.api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "    api_key = \"8e5931bade634c4e859a8e7544f87ff7\"\n",
    "\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        token = response.text\n",
    "        return token\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def request_tts(text):\n",
    "    endpoint = \"https://eastus.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "    token = get_token()\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"User-Agent\": \"testForEducation\",\n",
    "        \"X-Microsoft-OutputFormat\": \"riff-24khz-16bit-mono-pcm\",\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    "    data = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'><voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-JiMinNeural'>\n",
    "            {text}\n",
    "        </voice></speak>\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoint,\n",
    "                            headers=headers,\n",
    "                            data=data)\n",
    "    print(response)\n",
    "    if response.status_code == 200:\n",
    "        file_name = 'response_audio.wav'\n",
    "\n",
    "        with open(file_name, \"wb\") as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "\n",
    "        return file_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# request_tts('안녕하세요 저는 AI 챗봇입니다.')\n",
    "#############################################################\n",
    "\n",
    "def stream_webcam(image):\n",
    "    return detect_objects(image)\n",
    "\n",
    "def click_capture(image):\n",
    "    return image\n",
    "\n",
    "def click_send_gpt(image_array, history):\n",
    "    return chatgpt_response(image_array, history)\n",
    "\n",
    "def change_chatbot(chatbot):\n",
    "    import re\n",
    "    text = chatbot[-1][1]\n",
    "    pattern = r'[^가-힣a-zA-Z0-9\\s]'\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    file_name = request_tts(cleaned_text)\n",
    "    return file_name\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.Markdown(\"# Fimtrus's AI World!!!\")\n",
    "\n",
    "    with gr.Column():\n",
    "\n",
    "        with gr.Row():\n",
    "            webcam_input = gr.Image(label=\"실시간 화면\", sources=\"webcam\")\n",
    "            output_image = gr.Image(label=\"실시간 감지\", interactive=False)\n",
    "            output_capture_image = gr.Image(label=\"캡쳐 화면\", interactive=False)\n",
    "\n",
    "        with gr.Row():\n",
    "            capture_button = gr.Button('캡쳐')\n",
    "            send_gpt_button = gr.Button('GPT로 전송')\n",
    "\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot(label=\"분석 결과\")\n",
    "        chatbot_audio = gr.Audio(label='GPT', interactive=False, autoplay=True)\n",
    "        # chatbot, audio \n",
    "    \n",
    "    webcam_input.stream(fn=stream_webcam, inputs=[webcam_input], outputs=[output_image])\n",
    "    capture_button.click(fn=click_capture, inputs=[output_image], outputs=[output_capture_image])\n",
    "    send_gpt_button.click(fn=click_send_gpt, inputs=[output_capture_image, chatbot], outputs=[chatbot])\n",
    "    chatbot.change(fn=change_chatbot, inputs=[chatbot], outputs=[chatbot_audio])\n",
    "    # 실시간 화면에 대한 stream event.\n",
    "    #각종 이벤트 리스너 필요.\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 로직\n",
    "\n",
    "# 실시간 화면을 스트리밍 했을때, 실시간 감지화면에 감지된 이미지가 보여야하고,\n",
    "# 내가 원하는 이미지를 캡쳐버튼을 통해, 캡쳐화면에 보여준다.\n",
    "\n",
    "# GPT 버튼을 눌러서, OpenAI 로 이미지를 전송하는데, \n",
    "# System 메시지: 이미지를 분석하는 챗봇\n",
    "# User 메시지: 감지된 이미지를 분석\n",
    "# 캡쳐된 이미지를 포함, OpenAI로 전송\n",
    "\n",
    "# Response 받은 데이터를 챗봇 화면에 보여준다.\n",
    "\n",
    "# 챗봇에 어떤 텍스트가 업데이트 되었을 때, tts를 통해서, 음성파일을 받아온다.\n",
    "\n",
    "# operation\n",
    "def detect_objects():\n",
    "    # YOLO 모델을 불러와서 객체화\n",
    "    # 이미지를 받아와서\n",
    "    # 분석후\n",
    "    # Rectangle을 그려주고\n",
    "    # 레이블을 그려준다.\n",
    "    pass\n",
    "\n",
    "def request_gpt():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# event\n",
    "def click_capture():\n",
    "    pass\n",
    "\n",
    "def click_gpt():\n",
    "    pass\n",
    "\n",
    "\n",
    "## 화면\n",
    "# with gr.Blocks() as demo:\n",
    "#     with gr.Row():\n",
    "#         gr.Image()\n",
    "#         gr.Image()\n",
    "#         gr.Image()\n",
    "\n",
    "#     gr.Chatbot()\n",
    "#     gr.Audio()\n",
    "\n",
    "# 실시간 화면, 실시간 감지화면, 캡쳐화면\n",
    "# 캡쳐버튼, GPT 로 전송하는 버튼\n",
    "# 챗봇 \n",
    "# 오디오 화면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
